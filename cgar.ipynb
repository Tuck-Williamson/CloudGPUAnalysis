{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Record Keeping\n",
    "To keep good records of the systems that we test on start by dumping all of the information about the system to be sent to further processing.\n",
    "\n"
   ],
   "metadata": {
    "id": "xBD5EwoxGZRT"
   },
   "id": "xBD5EwoxGZRT"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xA6ypJm4boML",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dd1f1d1d-2b8e-4d93-c0bf-c76061596976"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n",
      "Host configuration documented.\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "\n",
    "# Setup directories.\n",
    "curDir = pathlib.Path(pathlib.Path.cwd()).resolve()\n",
    "workDir = curDir.joinpath(\"cgar\")\n",
    "workDir.mkdir(parents=True,exist_ok=True)\n",
    "!cd {workDir.resolve()}\n",
    "\n",
    "hostInfoDir = workDir.joinpath(\"hostInfo\")\n",
    "hostInfoDir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ErrorInfo = workDir.joinpath(\"ErrorLog.txt\")\n",
    "!touch {ErrorInfo}\n",
    "\n",
    "ProviderInfo = hostInfoDir.joinpath(\"ccPInfo.txt\")\n",
    "ProviderInfo.touch(600)\n",
    "\n",
    "#Set the variable indicating that this isn't running in the cloud.\n",
    "MyLocalMachine = os.environ.get('THW_MACHINE') is not None\n",
    "VastAI = os.environ.get('VAST_CONTAINERLABEL') is not None\n",
    "if VastAI:\n",
    "\n",
    "  !pip install vastai\n",
    "  !vastai start instance $CONTAINER_ID --api-key $CONTAINER_API_KEY\n",
    "  if _exit_code != 0:\n",
    "    # TODO: Log error!\n",
    "    !echo \"Error - This appears to be running in a Vast.ai instance, but the CLI threw an error: \" {_exit_code} >> {ErrorInfo}\n",
    "    # ErrorInfo\n",
    "\n",
    "!uname -a > {ProviderInfo} || echo \"Error getting Provider Info(uname)!\" >> {ErrorInfo}\n",
    "\n",
    "!echo -e \"### /etc/os-release ###\\n\\n\" >> {ProviderInfo}\n",
    "!cat /etc/os-release >> {ProviderInfo}\n",
    "!echo -e \"### /proc/version ###\\n\\n\" >> {ProviderInfo}\n",
    "!cat /proc/version >> {ProviderInfo}\n",
    "!echo -e \"### uptime ###\\n\\n\" >> {ProviderInfo}\n",
    "!uptime >> {ProviderInfo} || echo \"Error getting Provider Info(uptime)!\" >> {ErrorInfo}\n",
    "!echo -e \"### environment ###\\n\\n\" >> {ProviderInfo}\n",
    "!env >> {ProviderInfo}\n",
    "!echo -e \"### cuda-gdb version ###\\n\\n\" >> {ProviderInfo}\n",
    "!cuda-gdb --version >> {ProviderInfo} || echo \"Error getting CPU Info(cuda-gdb)!\" >> {ErrorInfo}\n",
    "\n",
    "# !command -v gcloud &> /dev/null\n",
    "# if _exit_code == 0:\n",
    "#   !echo -e \"### gcloud info ###\\n\\n\" >> {ProviderInfo}\n",
    "#   !cuda-gdb --version >> {ProviderInfo}\n",
    "\n",
    "# !cat {ProviderInfo}\n",
    "\n",
    "CPUInfo = hostInfoDir.joinpath(\"ccCInfo.txt\")\n",
    "# Get CPU info in JSON\n",
    "!lscpu -J > {CPUInfo} || echo \"Error getting CPU Info(lscpu)!\" >> {ErrorInfo}\n",
    "\n",
    "GPUInfo = hostInfoDir.joinpath(\"ccGInfo.txt\")\n",
    "# Get GPU info\n",
    "!nvidia-smi -q -x > {GPUInfo} || echo \"Error getting CPU Info(nvidia-smi)!\" >> {ErrorInfo}\n",
    "\n",
    "print('Host configuration documented.')\n",
    "def disconnect():\n",
    "  if not MyLocalMachine:\n",
    "    from google.colab import runtime\n",
    "    runtime.unassign()\n",
    "\n",
    "buildDir = workDir.joinpath(\"build\")\n",
    "buildDir.mkdir(parents=True,exist_ok=True)\n",
    "kernelPath = buildDir.joinpath(\"CGARKernel.cu\")\n",
    "!cd {buildDir}\n",
    "\n",
    "# !tar -cjf cgar-data-bundle.bz2 ./*\n",
    "if VastAI:\n",
    "  !vastai destroy instance $CONTAINER_ID --api-key $CONTAINER_API_KEY"
   ],
   "id": "xA6ypJm4boML"
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile test.ipy\n",
    "\n",
    "import os, pathlib\n",
    "\n",
    "# Setup directories.\n",
    "curDir = pathlib.Path(pathlib.Path.cwd()).resolve()\n",
    "workDir = curDir.joinpath(\"cgar\")\n",
    "workDir.mkdir(parents=True,exist_ok=True)\n",
    "!cd {workDir.resolve()}\n",
    "\n",
    "hostInfoDir = workDir.joinpath(\"hostInfo\")\n",
    "hostInfoDir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ErrorInfo = workDir.joinpath(\"ErrorLog.txt\")\n",
    "!touch {ErrorInfo}\n",
    "\n",
    "ProviderInfo = hostInfoDir.joinpath(\"ccPInfo.txt\")\n",
    "ProviderInfo.touch(600)\n",
    "\n",
    "#Set the variable indicating that this isn't running in the cloud.\n",
    "MyLocalMachine = os.environ.get('THW_MACHINE') is not None\n",
    "VastAI = os.environ.get('VAST_CONTAINERLABEL') is not None\n",
    "if VastAI:\n",
    "\n",
    "  !pip install vastai\n",
    "  !vastai start instance $CONTAINER_ID --api-key $CONTAINER_API_KEY\n",
    "  if _exit_code != 0:\n",
    "    # TODO: Log error!\n",
    "    !echo \"Error - This appears to be running in a Vast.ai instance, but the CLI threw an error: \" {_exit_code} >> {ErrorInfo}\n",
    "    # ErrorInfo\n",
    "\n",
    "!uname -a > {ProviderInfo} || echo \"Error getting Provider Info(uname)!\" >> {ErrorInfo}\n",
    "\n",
    "!echo -e \"### /etc/os-release ###\\n\\n\" >> {ProviderInfo}\n",
    "!cat /etc/os-release >> {ProviderInfo}\n",
    "!echo -e \"### /proc/version ###\\n\\n\" >> {ProviderInfo}\n",
    "!cat /proc/version >> {ProviderInfo}\n",
    "!echo -e \"### uptime ###\\n\\n\" >> {ProviderInfo}\n",
    "!uptime >> {ProviderInfo} || echo \"Error getting Provider Info(uptime)!\" >> {ErrorInfo}\n",
    "!echo -e \"### environment ###\\n\\n\" >> {ProviderInfo}\n",
    "!env >> {ProviderInfo}\n",
    "!echo -e \"### cuda-gdb version ###\\n\\n\" >> {ProviderInfo}\n",
    "!cuda-gdb --version >> {ProviderInfo} || echo \"Error getting CPU Info(cuda-gdb)!\" >> {ErrorInfo}\n",
    "\n",
    "# !command -v gcloud &> /dev/null\n",
    "# if _exit_code == 0:\n",
    "#   !echo -e \"### gcloud info ###\\n\\n\" >> {ProviderInfo}\n",
    "#   !cuda-gdb --version >> {ProviderInfo}\n",
    "\n",
    "# !cat {ProviderInfo}\n",
    "\n",
    "CPUInfo = hostInfoDir.joinpath(\"ccCInfo.txt\")\n",
    "# Get CPU info in JSON\n",
    "!lscpu -J > {CPUInfo} || echo \"Error getting CPU Info(lscpu)!\" >> {ErrorInfo}\n",
    "\n",
    "GPUInfo = hostInfoDir.joinpath(\"ccGInfo.txt\")\n",
    "# Get GPU info\n",
    "!nvidia-smi -q -x > {GPUInfo} || echo \"Error getting CPU Info(nvidia-smi)!\" >> {ErrorInfo}\n",
    "\n",
    "print('Host configuration documented.')\n",
    "def disconnect():\n",
    "  if VastAI:\n",
    "    !vastai destroy instance $CONTAINER_ID --api-key $CONTAINER_API_KEY\n",
    "  if not MyLocalMachine:\n",
    "    from google.colab import runtime\n",
    "    runtime.unassign()\n",
    "\n",
    "buildDir = workDir.joinpath(\"build\")\n",
    "buildDir.mkdir(parents=True,exist_ok=True)\n",
    "kernelPath = buildDir.joinpath(\"CGARKernel.cu\")\n",
    "!cd {buildDir}\n",
    "\n",
    "\n",
    "# !tar -cjf cgar-data-bundle.bz2 ./*"
   ],
   "metadata": {
    "id": "ymTcQtiS5e6g"
   },
   "id": "ymTcQtiS5e6g",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup Build Sources"
   ],
   "metadata": {
    "id": "7aRHigsYbBOT"
   },
   "id": "7aRHigsYbBOT"
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile $kernelPath\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <unistd.h>\n",
    "#include <cuda.h>\n",
    "\n",
    "__global__ void cgar_kernel(){\n",
    "            printf(\"Hello World from GPU!\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "\n",
    "  int deviceCount = 0;\n",
    "  cuDeviceGetCount(&deviceCount);\n",
    "  if (deviceCount == 0) {\n",
    "      printf(\"There is no device supporting CUDA.\\n\");\n",
    "      exit (0);\n",
    "  }\n",
    "\n",
    "  // Get handle for device 0\n",
    "  CUdevice cuDevice;\n",
    "  cuDeviceGet(&cuDevice, 0);\n",
    "\n",
    "  // Create context\n",
    "  CUcontext cuContext;\n",
    "  cuCtxCreate(&cuContext, 0, cuDevice);\n",
    "  while(true){\n",
    "    sleep(30);\n",
    "  }\n",
    "  cgar_kernel<<<1,1>>>();\n",
    "  return 0;\n",
    "}"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z1CBwbKObFru",
    "outputId": "7230acfd-9afb-46bf-c0d9-9ebd7748f9aa"
   },
   "id": "z1CBwbKObFru",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing /content/build/CGARKernel.cu\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "os.environ['CUDA_DEBUGGER_SOFTWARE_PREEMPTION']='1'\n",
    "!!cd $buildDir\n",
    "# nvcc --help\n",
    "# !nvcc -g -G -c $kernelPath --generate-dependencies-with-compile --dependency-output $kernelPath.stem.d -o $kernelPath.stem\n",
    "!nvcc -g -G $kernelPath -o $kernelPath.stem\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36zytZTNfAgQ",
    "outputId": "234663e9-d2e9-4a02-c44b-986b44deb842"
   },
   "id": "36zytZTNfAgQ",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/usr/bin/ld: /tmp/tmpxft_00000320_00000000-11_CGARKernel.o: in function `main':\n",
      "/content/build/CGARKernel.cu:13: undefined reference to `cuDeviceGetCount'\n",
      "/usr/bin/ld: /content/build/CGARKernel.cu:21: undefined reference to `cuDeviceGet'\n",
      "/usr/bin/ld: /content/build/CGARKernel.cu:25: undefined reference to `cuCtxCreate_v2'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile cuda_test.cu\n",
    "//From https://developer.nvidia.com/blog/even-easier-introduction-cuda/\n",
    "#include <cuda.h>\n",
    "#include <iostream>\n",
    "#include <iomanip>\n",
    "\n",
    "// Kernel function to add the elements of two arrays\n",
    "__global__ void validation_gen(const unsigned int maxIdx, unsigned int* dataPtr)\n",
    "{\n",
    "  int index = threadIdx.x;\n",
    "  int stride = blockDim.x;\n",
    "  for (int idx = index; idx < maxIdx; idx += stride)\n",
    "      dataPtr[idx] = stride << 16 & index;\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "  unsigned int N = 256<<2;\n",
    "  unsigned int* dataPtr;\n",
    "\n",
    "  // Allocate Unified Memory - accessible from CPU or GPU\n",
    "  cudaMallocManaged(&dataPtr, N*sizeof(dataPtr[0]));\n",
    "\n",
    "  // Run kernel on 1M elements on the GPU\n",
    "  validation_gen<<<1, 256>>>(N, dataPtr);\n",
    "\n",
    "  // Wait for GPU to finish before accessing on host\n",
    "  cudaDeviceSynchronize();\n",
    "\n",
    "  std::cout << std::hex;\n",
    "  for(uint i = 0; i < N; i++){\n",
    "    auto idx = i & 0xffff80;\n",
    "    auto stride = i & 0x7f;\n",
    "    stride = stride << 16-5;\n",
    "\n",
    "    std::cout << \"(\" << std::dec << i << \"): \" << std::hex << dataPtr[i] << \" -> \" << stride << idx;\n",
    "    if(i % 4 == 0){\n",
    "        std::cout << std::endl;\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Free memory\n",
    "  cudaFree(dataPtr);\n",
    "\n",
    "  return 0;\n",
    "}"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pC3r3rlBWfo_",
    "outputId": "7c5887ca-36fe-4537-cb5a-a395d58cc83b"
   },
   "id": "pC3r3rlBWfo_",
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting cuda_test.cu\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!nvcc cuda_test.cu -o cuda_validator"
   ],
   "metadata": {
    "id": "Je48k1w6vIa_"
   },
   "id": "Je48k1w6vIa_",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile cuda_tester.cu\n",
    "\n",
    "#include <cuda.h>\n",
    "#include <stdio.h>\n",
    "#include <iostream>\n",
    "#include <algorithm>\n",
    "\n",
    "void errChk(cudaError_t status, size_t line){\n",
    "    if(status != cudaSuccess){\n",
    "        std::cerr << \"There was a cuda error at line \" << line << \".\" << std::endl;\n",
    "        std::cerr << \"Error (\" << status << \"): \" << cudaGetErrorName(status) << \"::\" << cudaGetErrorString(status) << std::endl;\n",
    "        throw 1;\n",
    "    }\n",
    "}\n",
    "\n",
    "#define MByte 1024*1024\n",
    "\n",
    "__global__ void validationGen(const unsigned int* endPtr, unsigned int* basePtr)\n",
    "{\n",
    "  auto index = threadIdx.x;\n",
    "  auto stride = blockDim.x;\n",
    "  for (int i = index; i < n; i += stride)\n",
    "      y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "\n",
    "    cudaError_t cuErr;\n",
    "\n",
    "    cudaDeviceProp props;\n",
    "    errChk(cudaGetDeviceProperties(&props, 0), __LINE__);\n",
    "\n",
    "    size_t totalMem = props.totalGlobalMem;\n",
    "\n",
    "    void* devPtr;\n",
    "    std::cout << \"Trying to allocate \" << totalMem/MByte << \" M bytes.\" << std::endl;\n",
    "    while(cudaMalloc(&devPtr, totalMem) != cudaSuccess)\n",
    "    {\n",
    "        cuErr = cudaPeekAtLastError();\n",
    "        if(totalMem < 5*MByte || cuErr != cudaErrorMemoryAllocation){\n",
    "            std::cerr << \"There was a cuda error when allocating \" << totalMem << \" bytes.\" << std::endl;\n",
    "            errChk(cuErr, __LINE__);\n",
    "            return 2;\n",
    "        }\n",
    "        totalMem -= MByte;\n",
    "        std::cout << \"Trying to allocate \" << totalMem/MByte << \" M bytes.\" << std::endl;\n",
    "    }\n",
    "    std::cout << \"Successfully allocated \" << totalMem << \" bytes.\" << std::endl;\n",
    "\n",
    "\n",
    "    char* hostPtr = (char*)malloc(totalMem);\n",
    "    try{\n",
    "        errChk(cudaMemcpy((void*)hostPtr, devPtr, totalMem, cudaMemcpyDeviceToHost), __LINE__);\n",
    "\n",
    "        std::cout << \"Read \" << totalMem << \" bytes from GPU memory.\" << std::endl;\n",
    "\n",
    "        errChk(cudaFree(devPtr), __LINE__);\n",
    "\n",
    "        std::cout << \"Writing \" << totalMem << \" bytes to gdump.bin file.\" << std::endl;\n",
    "        FILE* dumpFilePtr = fopen(\"gdump.bin\", \"wb\");\n",
    "        if(dumpFilePtr == NULL){\n",
    "            std::cerr << \"Error opening 'gdump.bin'!\" << std::endl;\n",
    "            return 1;\n",
    "        }\n",
    "\n",
    "        // Optimizations for sparse file generation.\n",
    "\n",
    "        //Check larger sections using 64b\n",
    "        unsigned long long* startDataPtr = reinterpret_cast<unsigned long long*>(hostPtr);\n",
    "        unsigned long long* endHostPtr = reinterpret_cast<unsigned long long*>(hostPtr+totalMem);\n",
    "        unsigned long long* endDataPtr = startDataPtr;\n",
    "\n",
    "        while(endDataPtr < endHostPtr){\n",
    "          //Get ptr to first non-zero data.\n",
    "          startDataPtr = std::find_if(startDataPtr,\n",
    "                                endHostPtr, [](auto datum){ return datum != 0;});\n",
    "          //Get ptr to first zero data after non-zero data.\n",
    "          endDataPtr = std::find_if(static_cast<unsigned long long*>(startDataPtr),\n",
    "                                endHostPtr, [](auto datum){ return datum == 0;});\n",
    "\n",
    "          std::cout << \"  Writing \" << (endDataPtr - startDataPtr) * sizeof(*endDataPtr)\n",
    "                << \" bytes offset by \" << (char*)startDataPtr - hostPtr << \" from \" << startDataPtr << \" to \" << endDataPtr << \".\" << std::endl;\n",
    "          fseek(dumpFilePtr, (char*)startDataPtr - hostPtr, SEEK_SET);\n",
    "          fwrite(startDataPtr, sizeof(*startDataPtr), endDataPtr-startDataPtr, dumpFilePtr);\n",
    "          fflush(dumpFilePtr);\n",
    "        }\n",
    "        //fwrite(hostPtr, 1, totalMem, dumpFilePtr);\n",
    "\n",
    "        fclose(dumpFilePtr);\n",
    "\n",
    "        free(hostPtr);\n",
    "    }\n",
    "    catch(int err){\n",
    "        cudaFree(devPtr);\n",
    "        free(hostPtr);\n",
    "        return 1;\n",
    "    }\n",
    "    return 0;\n",
    "}"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fde3a0ab-8ac7-4020-8312-705981248c4e",
    "id": "kuC4YsZ01Fom"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing cuda_test.cu\n"
     ]
    }
   ],
   "id": "kuC4YsZ01Fom"
  },
  {
   "cell_type": "code",
   "source": [
    "os.environ['CUDA_DEBUGGER_SOFTWARE_PREEMPTION']='1'\n",
    "!cd /content/\n",
    "# nvcc --help\n",
    "# !nvcc -g -G -c $kernelPath --generate-dependencies-with-compile --dependency-output $kernelPath.stem.d -o $kernelPath.stem\n",
    "!nvcc -g -G basic_read.cu -o basic_read"
   ],
   "metadata": {
    "id": "wegbeTiirUwI"
   },
   "execution_count": null,
   "outputs": [],
   "id": "wegbeTiirUwI"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "disconnect()"
   ],
   "metadata": {
    "id": "gFF01CBNt6Bq"
   },
   "id": "gFF01CBNt6Bq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Promising info\n",
    "\n",
    "https://docs.nvidia.com/cuda/cuda-gdb/#variable-storage-and-accessibility\n",
    "\n",
    "https://forums.developer.nvidia.com/t/gpu-memory-dump-with-cuda-gdb-python/46226/2\n",
    "\n"
   ],
   "metadata": {
    "id": "mBubY4YwZ3Ah"
   },
   "id": "mBubY4YwZ3Ah"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Python Approach\n",
    "\n",
    "Maybe this can be combined with `cuda-gdb` to simplify the process."
   ],
   "metadata": {
    "id": "4-1C8fqC0WOi"
   },
   "id": "4-1C8fqC0WOi"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install cuda-python\n",
    "from cuda import cuda, nvrtc"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KxDXjrp0VTh",
    "outputId": "adacd703-8b39-40c5-df84-ba2c1b3db6fe"
   },
   "id": "4KxDXjrp0VTh",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: cuda-python in /usr/local/lib/python3.10/dist-packages (12.2.1)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python) (3.0.11)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from cuda import cuda, nvrtc\n",
    "import time\n",
    "\n",
    "# Much of this is from the cuda-python docs at https://nvidia.github.io/cuda-python/overview.html\n",
    "\n",
    "def _cudaGetErrorEnum(error):\n",
    "    if isinstance(error, cuda.CUresult):\n",
    "        err, name = cuda.cuGetErrorName(error)\n",
    "        return name if err == cuda.CUresult.CUDA_SUCCESS else \"<unknown>\"\n",
    "    elif isinstance(error, nvrtc.nvrtcResult):\n",
    "        return nvrtc.nvrtcGetErrorString(error)[1]\n",
    "    else:\n",
    "        raise RuntimeError('Unknown error type: {}'.format(error))\n",
    "\n",
    "def errChk(result):\n",
    "    if result[0].value:\n",
    "        raise RuntimeError(\"CUDA error code={}({})\".format(result[0].value, _cudaGetErrorEnum(result[0])))\n",
    "    if len(result) == 1:\n",
    "        return None\n",
    "    elif len(result) == 2:\n",
    "        return result[1]\n",
    "    else:\n",
    "        return result[1:]\n",
    "\n",
    "errChk(cuda.cuInit(0))\n",
    "\n",
    "numDevices = errChk(cuda.cuDeviceGetCount())\n",
    "if numDevices < 1:\n",
    "  raise RuntimeError(\"No CUDA devices.\")\n",
    "else:\n",
    "  print(f\"There are {numDevices} CUDA devices.\")\n",
    "\n",
    "for devIdx in range(numDevices):\n",
    "  print(errChk(cuda.cuDeviceGetProperties(devIdx)))\n",
    "\n",
    "#TODO decide if we look at all of the devices????\n",
    "\n",
    "dev = errChk(cuda.cuDeviceGet(0))\n",
    "\n",
    "context = errChk(cuda.cuCtxCreate(0, dev))\n",
    "try:\n",
    "  cuda.cuMemGetInfo()\n",
    "  waitCount = 10\n",
    "  print('Starting wait loop.')\n",
    "  while waitCount >= 0:\n",
    "    print('.')\n",
    "    waitCount -= 1\n",
    "    time.sleep(60)\n",
    "finally:\n",
    "  errChk(cuda.cuCtxDestroy(context))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "oiniTsBv00im",
    "outputId": "c00da172-cdd0-4c12-ca46-f951e817d11b"
   },
   "id": "oiniTsBv00im",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Failed to dlopen libcuda.so.1",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-45ffc0750f56>\u001B[0m in \u001B[0;36m<cell line: 25>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m \u001B[0merrChk\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuInit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0mnumDevices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0merrChk\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuDeviceGetCount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/cuda/cuda.pyx\u001B[0m in \u001B[0;36mcuda.cuda.cuInit\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/cuda/ccuda.pyx\u001B[0m in \u001B[0;36mcuda.ccuda.cuInit\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/cuda/_cuda/ccuda.pyx\u001B[0m in \u001B[0;36mcuda._cuda.ccuda._cuInit\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/cuda/_cuda/ccuda.pyx\u001B[0m in \u001B[0;36mcuda._cuda.ccuda.cuPythonInit\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to dlopen libcuda.so.1"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Old CUDA Setup"
   ],
   "metadata": {
    "id": "tywXSuiF8iXf"
   },
   "id": "tywXSuiF8iXf"
  },
  {
   "cell_type": "code",
   "source": [
    "# import torch, os, math, gzip, pickle\n",
    "# import matplotlib.pyplot as plt\n",
    "# from urllib.request import urlretrieve\n",
    "# from pathlib import Path\n",
    "\n",
    "# from torch import tensor\n",
    "# import torchvision as tv\n",
    "# import torchvision.transforms.functional as tvf\n",
    "# from torchvision import io\n",
    "# from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "# %pip install -q wurlitzer ninja\n",
    "# %load_ext wurlitzer\n",
    "\n",
    "# def load_cuda(cuda_src, cpp_src, funcs, opt=False, verbose=False):\n",
    "#     return load_inline(cuda_sources=[cuda_src], cpp_sources=[cpp_src], functions=funcs,\n",
    "#                        extra_cuda_cflags=[\"-O2\"] if opt else [], verbose=verbose,\n",
    "#                        name=\"inline_ext\", build_directory=buildDir, keep_intermediates=True)\n"
   ],
   "metadata": {
    "id": "eHjODuGrDOXo"
   },
   "id": "eHjODuGrDOXo",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "# buildDir = os.curdir + \"/build\"\n",
    "# !mkdir -p {buildDir}\n",
    "\n",
    "# cuda_begin = r'''\n",
    "# #include <torch/extension.h>\n",
    "# #include <stdio.h>\n",
    "# #include <unistd.h>\n",
    "# #include <c10/cuda/CUDAException.h>\n",
    "\n",
    "# #define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
    "# #define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "# #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "\n",
    "# inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
    "# '''\n",
    "\n",
    "# cuda_src = cuda_begin + r'''\n",
    "# __global__ void cuda_test_kernel(unsigned char* x, int n) {\n",
    "#     int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "\n",
    "#     int test;\n",
    "#     if (i<n) {\n",
    "#       test = 0.2989*x[i] + 0.5870*x[i+n] + 0.1140*x[i+2*n];\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# int cuda_test_host(int n) {\n",
    "#     // CHECK_INPUT(input);\n",
    "#     //int h = input.size(1);\n",
    "#     //int w = input.size(2);\n",
    "#     //printf(\"h*w: %d*%d\\n\", h, w);\n",
    "#     //auto output = torch::empty({h,w}, input.options());\n",
    "#     //int threads = 256;\n",
    "\n",
    "#     while(1==1){\n",
    "#       sleep(30);\n",
    "#     }\n",
    "#     unsigned char testArray[100];\n",
    "#     testArray[0] = 'h';\n",
    "\n",
    "#     cuda_test_kernel<<<1, 1>>>(&testArray[0], 0);\n",
    "#     C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "#     return 1;\n",
    "# }'''\n",
    "\n",
    "# cpp_src = \"int cuda_test_host(int n);\"\n",
    "\n",
    "# module = load_cuda(cuda_src, cpp_src, ['cuda_test_host'], verbose=True)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXv8pL2V8pvE",
    "outputId": "d27ca9e0-1921-4fb5-9f01-71140f392db8"
   },
   "id": "lXv8pL2V8pvE",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The input conditions for extension module inline_ext have changed. Bumping to version 1 and re-building as inline_ext_v1...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file ./build/build.ninja...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module inline_ext_v1...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /content/build/main.cpp -o main.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -std=c++17 -c /content/build/cuda.cu -o cuda.cuda.o \n",
      "/content/build/cuda.cu(19): warning #550-D: variable \"test\" was set but never used\n",
      "      int test;\n",
      "          ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "[3/3] c++ main.o cuda.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v1.so\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading extension module inline_ext_v1...\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
